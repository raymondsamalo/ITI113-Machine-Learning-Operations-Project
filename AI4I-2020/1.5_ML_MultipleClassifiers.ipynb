{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cd1b641a",
   "metadata": {},
   "source": [
    "# Multiple classifiers\n",
    "\n",
    "We will attempt to create methods to train and evaluate multiple multi-output classifier and rank them based on F2 score\n",
    "\n",
    "Given we were not able to create a good balanced dataset, we will test using original dataset with reduced features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "db12136a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: xgboost in /opt/anaconda3/envs/ml/lib/python3.13/site-packages (3.0.2)\n",
      "Requirement already satisfied: numpy in /opt/anaconda3/envs/ml/lib/python3.13/site-packages (from xgboost) (1.26.4)\n",
      "Requirement already satisfied: scipy in /opt/anaconda3/envs/ml/lib/python3.13/site-packages (from xgboost) (1.15.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "bd304f03",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import xgboost as xb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3928ea80",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train=pd.read_csv('X_train.csv')\n",
    "y_train=pd.read_csv('y_train.csv')\n",
    "X_test=pd.read_csv('X_test.csv')\n",
    "y_test=pd.read_csv('y_test.csv')\n",
    "y_train_omf = y_train['Machine failure']\n",
    "y_test_omf = y_test['Machine failure']\n",
    "X_train_reduced = X_train.drop(columns=['Torque _Nm_', 'Process temperature _K_', 'Air temperature _K_'])\n",
    "X_test_reduced = X_test.drop(columns=['Torque _Nm_', 'Process temperature _K_', 'Air temperature _K_'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2716d9f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import fbeta_score, precision_score, recall_score, accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "\n",
    "def get_ml_perf_machine_failure(name, clf, X_train=X_train, y_train=y_train, X_test=X_test, y_test=y_test):\n",
    "    y_train_omf = y_train['Machine failure']\n",
    "    y_test_omf = y_test['Machine failure']\n",
    "    clf = MultiOutputClassifier(clf)\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_train_pred= clf.predict(X_train)\n",
    "    y_train_pred = pd.DataFrame(y_train_pred, columns=y_test.columns)\n",
    "    y_test_pred = clf.predict(X_test)\n",
    "    y_test_pred = pd.DataFrame(y_test_pred, columns=y_test.columns)\n",
    "    y_test_pred_omf = y_test_pred['Machine failure']\n",
    "    y_train_pred_omf = y_train_pred['Machine failure']\n",
    "    f2_train=fbeta_score(y_train_omf, y_train_pred_omf, beta=2)\n",
    "    f2_test=fbeta_score(y_test_omf, y_test_pred_omf, beta=2)\n",
    "    train_recall=recall_score(y_train_omf, y_train_pred_omf)\n",
    "    test_recall=recall_score(y_test_omf, y_test_pred_omf)\n",
    "    train_precision=precision_score(y_train_omf, y_train_pred_omf, zero_division=0)\n",
    "    test_precision=precision_score(y_test_omf, y_test_pred_omf, zero_division=0)\n",
    "    train_accuracy=accuracy_score(y_train_omf, y_train_pred_omf)\n",
    "    test_accuracy=accuracy_score(y_test_omf, y_test_pred_omf)\n",
    "    predictor_name = clf.__class__.__name__\n",
    "    cm = confusion_matrix(y_train_omf, y_train_pred_omf)\n",
    "    train_tn, train_fp, train_fn, train_tp = cm.ravel()\n",
    "    cm = confusion_matrix(y_test_omf, y_test_pred_omf)\n",
    "    test_tn, test_fp, test_fn, test_tp = cm.ravel()\n",
    "    result = {\n",
    "        \"multi-output-classifier\": clf,\n",
    "        \"f2_train\": f2_train,\n",
    "        \"f2_test\": f2_test,\n",
    "        \"train_accuracy\": train_accuracy,\n",
    "        \"test_accuracy\": test_accuracy,\n",
    "        \"train_recall\":train_recall,\n",
    "        \"test_recall\": test_recall,\n",
    "        \"test_precision\": test_precision,\n",
    "        \"train_precision\":train_precision,\n",
    "        \"predictor_name\": predictor_name,\n",
    "        \"train_fp\": train_fp,\n",
    "        \"train_fn\": train_fn,\n",
    "        \"test_fp\": test_fp,\n",
    "        \"test_fn\": test_fn,\n",
    "        \"train_tp\": train_tp,\n",
    "        \"train_tn\": train_tn,\n",
    "        \"test_tp\": test_tp,\n",
    "        \"test_tn\": test_tn,\n",
    "        \"test_pred\": y_test_pred,\n",
    "        \"train_pred\": y_train_pred,\n",
    "        \"name\": name        \n",
    "    }\n",
    "    return result\n",
    "\n",
    "def evaluate_models(models:dict,  X_train=X_train, y_train=y_train, X_test=X_test, y_test=y_test):\n",
    "    results = []\n",
    "    for name, model in models.items():\n",
    "        results.append(get_ml_perf_machine_failure(name, model, X_train=X_train, y_train=y_train, X_test=X_test, y_test=y_test))\n",
    "    results.sort(reverse=True, key=lambda result: result[\"f2_test\"])\n",
    "    print(\" -- Sorted result --\")\n",
    "    for result in results:\n",
    "        print(f\"{result[\"name\"]:20} : f2-test {result[\"f2_test\"]:20} | accuracy {result[\"test_accuracy\"]:10}  | test-fp {result[\"test_fp\"]:10}  | test-fn {result[\"test_fn\"]:10}\")\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e759889d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "on full features\n",
      " -- Sorted result --\n",
      "Decision Tree md 5   : f2-test   0.8840579710144928 | accuracy     0.9935  | test-fp          4  | test-fn          9\n",
      "Nearest Neighbors    : f2-test   0.8670520231213873 | accuracy      0.992  | test-fp          6  | test-fn         10\n",
      "Decision Tree md 10  : f2-test   0.8670520231213873 | accuracy      0.992  | test-fp          6  | test-fn         10\n",
      "Naive Bayes          : f2-test    0.855072463768116 | accuracy     0.9915  | test-fp          6  | test-fn         11\n",
      "AdaBoost             : f2-test   0.5376344086021505 | accuracy      0.959  | test-fp         52  | test-fn         30\n",
      "Random Forest        : f2-test   0.4716981132075472 | accuracy      0.976  | test-fp          8  | test-fn         40\n",
      "Logistic Regression  : f2-test  0.40192926045016075 | accuracy     0.9745  | test-fp          6  | test-fn         45\n",
      "on reduced features\n",
      " -- Sorted result --\n",
      "Decision Tree md 5   : f2-test   0.8918128654970761 | accuracy      0.995  | test-fp          1  | test-fn          9\n",
      "Decision Tree md 10  : f2-test   0.8746355685131195 | accuracy     0.9935  | test-fp          3  | test-fn         10\n",
      "Nearest Neighbors    : f2-test   0.8575581395348837 | accuracy      0.992  | test-fp          5  | test-fn         11\n",
      "Naive Bayes          : f2-test    0.847953216374269 | accuracy      0.992  | test-fp          4  | test-fn         12\n",
      "Random Forest        : f2-test   0.4716981132075472 | accuracy      0.976  | test-fp          8  | test-fn         40\n",
      "AdaBoost             : f2-test  0.41420118343195267 | accuracy      0.964  | test-fp         30  | test-fn         42\n",
      "Logistic Regression  : f2-test  0.40192926045016075 | accuracy     0.9745  | test-fp          6  | test-fn         45\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier, RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "names = [\n",
    "    \"Logistic Regression\",\n",
    "    \"Nearest Neighbors\",\n",
    "    \"Decision Tree md 10\",\n",
    "    \"Decision Tree md 5\",\n",
    "    \"Random Forest\",\n",
    "    \"AdaBoost\",\n",
    "    \"Naive Bayes\",\n",
    "    \"XGBoost\"\n",
    "]\n",
    "\n",
    "classifiers = [\n",
    "    KNeighborsClassifier(5),\n",
    "    DecisionTreeClassifier(random_state=42, max_depth=10, min_samples_split=10, min_samples_leaf=5),\n",
    "    DecisionTreeClassifier(random_state=42, max_depth=5, min_samples_split=10, min_samples_leaf=5),\n",
    "    RandomForestClassifier(random_state=42),\n",
    "    AdaBoostClassifier(random_state=42),\n",
    "    GaussianNB(),\n",
    "    XGBClassifier(\n",
    "        n_estimators=100,\n",
    "    objective='binary:logistic',\n",
    "    random_state=42)\n",
    "]\n",
    "\n",
    "models = {name: clf for name, clf in zip(names, classifiers)}\n",
    "\n",
    "print(\"on full features\")\n",
    "results = evaluate_models(models) # use reduced features\n",
    "\n",
    "print(\"on reduced features\")\n",
    "results_reduced = evaluate_models(models, X_train=X_train_reduced, X_test=X_test_reduced) # use reduced features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e467cd16",
   "metadata": {},
   "source": [
    "Decision Tree with max depth=5, Reduced feature performed the best among all of our machine learning models\n",
    "\n",
    "The hyper parameter max depth makes quite a difference, we should optimize our decision tree hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd07e2af",
   "metadata": {},
   "source": [
    "## Hyperparameters Optimization For Decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84d9a7ca",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c751c18d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'over_X_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[34]\u001b[39m\u001b[32m, line 14\u001b[39m\n\u001b[32m      8\u001b[39m param_grid = {\n\u001b[32m      9\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mmax_features\u001b[39m\u001b[33m'\u001b[39m: [\u001b[33m'\u001b[39m\u001b[33msqrt\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mlog2\u001b[39m\u001b[33m'\u001b[39m],\n\u001b[32m     10\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mmax_depth\u001b[39m\u001b[33m'\u001b[39m : [\u001b[32m4\u001b[39m, \u001b[32m6\u001b[39m, \u001b[32m8\u001b[39m, \u001b[32m10\u001b[39m, \u001b[32m12\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m],\n\u001b[32m     11\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mn_estimators\u001b[39m\u001b[33m'\u001b[39m: [\u001b[32m1\u001b[39m, \u001b[32m8\u001b[39m, \u001b[32m16\u001b[39m, \u001b[32m32\u001b[39m, \u001b[32m64\u001b[39m, \u001b[32m100\u001b[39m, \u001b[32m200\u001b[39m],\n\u001b[32m     12\u001b[39m }\n\u001b[32m     13\u001b[39m rf_clf = RandomForestClassifier(random_state=\u001b[32m42\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m rf_clf.fit(\u001b[43mover_X_train\u001b[49m, over_y_train)  \u001b[38;5;66;03m# fit the classifier on the training data\u001b[39;00m\n\u001b[32m     15\u001b[39m clf = GridSearchCV(rf_clf,param_grid = param_grid, cv = \u001b[32m3\u001b[39m, verbose=\u001b[38;5;28;01mFalse\u001b[39;00m, scoring=f2_scorer)\n\u001b[32m     16\u001b[39m best_rf_clf = clf.fit(over_X_train, over_y_train)\n",
      "\u001b[31mNameError\u001b[39m: name 'over_X_train' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from sklearn.metrics import make_scorer, fbeta_score\n",
    "f2_scorer = make_scorer(fbeta_score, beta=2)\n",
    "# Set rule to ignore warnings\n",
    "param_grid = {\n",
    "    'max_features': ['sqrt', 'log2'],\n",
    "    'max_depth' : [4, 6, 8, 10, 12, None],\n",
    "    'n_estimators': [1, 8, 16, 32, 64, 100, 200],\n",
    "}\n",
    "rf_clf = RandomForestClassifier(random_state=42)\n",
    "rf_clf.fit(over_X_train, over_y_train)  # fit the classifier on the training data\n",
    "clf = GridSearchCV(rf_clf,param_grid = param_grid, cv = 3, verbose=False, scoring=f2_scorer)\n",
    "best_rf_clf = clf.fit(over_X_train, over_y_train)\n",
    "best_rf_clf.best_estimator_\n",
    "print(\"Best parameters for Random Forest Regression:\")\n",
    "print(best_rf_clf.best_params_)\n",
    "print(\"Best score for Random Forest Regression:\")\n",
    "print(best_rf_clf.best_score_)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
